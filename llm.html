<html>
    <head>
        <style>
            h4{
                font-size: 36px;
            }
            figcaption{
                font-size: 10px;
            }
        </style>
    </head>
    <body>
        <div align="center">
            <h2>Large Language Models</h2>
        </div>
        <br>
        Large language models (LLMs) are designed to understand and generate human language.
        <br>
        When generating a response, LLMs assess the probability of all possible next words and pick the most likely next word most of the time.
        However, less likely next words are picked at random in order to ensure unique responses every time the question is asked.
        LLMs assess the probability of next words using large amounts of parameters (GPT-4 has over 1.7 trillion!) that dictate how such calculations should be made.
        All parameters start at a random value, and the algorithm is trained by showing it all words in each passage of training data except the last. The current parameters are used to guess the last word, and the best way to improve the algorithm is calculated using <a href="https://www.geeksforgeeks.org/machine-learning/backpropagation-in-neural-network/" target="_blank">backpropagation</a>.
        In the past few years, <a href="https://poloclub.github.io/transformer-explainer/" target="_blank">transformers</a> have allowed for the creation of better LLMs. They allow words in a sentence to look at every other word in the sentence to gain context and understand the order the words are placed in (<a href="https://www.geeksforgeeks.org/artificial-intelligence/ml-attention-mechanism/" target="_blank">attention mechanism</a>). The importance of each word can be adjusted depending on the context.
        <div align="center">
            <h3><a href="https://github.com/ericfan512/LLMChatInterface" target="_blank">An LLM Interface Project (using streamlit)</a></h3>
        </div>
        <h4><a href="index.html">Back</a></h4>
    </body>
</html>